---
title: "Articulo 3: Naive-Bayes"
authors:
  - name: Jorge Nevarez - A01645313
  - name: Alfredo Lopez - A01638976
  - name: Damian Luna - A01645865
  - name: Diana Hernandez - A01639515
format: html
editor: visual
---

```{r}
#Setup global cargando librerias y nuestra base de datos
install.packages("tidytext")
library(tidyverse)
library(tidymodels)
library(tidytext)
library(stringr)
library(ggplot2)
library(tidyr)
library(Matrix)

data <- read_csv("data/stories.csv")
head(data)
```

✏️ Realicen un pequeño análisis de texto utilizando la base de datos que scrapearon. La actividad de Mine sobre text mining les puede dar ideas y el código necesario para realizar este análisis.

```{r}
data_clean <- data %>%
  mutate(Description = str_to_lower(Description)) %>%
  mutate(Description = str_replace_all(Description, "[[:punct:]]", " ")) %>%
  mutate(Description = str_squish(Description))
```

### **Tokenización y remoción de stopwords**

```{r}
tokens <- data_clean %>%
  unnest_tokens(word, Description) %>%
  anti_join(stop_words, by = "word")
```

### **Frecuencia de palabras**

```{r}
top_words <- tokens %>%
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20)

top_words
```

### **Visualización**

```{r}
top_words %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 palabras en relatos",
       x = "Palabra", y = "Frecuencia") +
  theme_minimal()
```

✏️ Construyan una sparse matrix que incluya la frecuencia de cada palabra en los relatos scrapeados, donde cada renglón representa un relato diferente y donde las columnas son las diferentes palabras. Esta matriz también debe contener el tipo de evento paranormal. La función unnest_tokens() les puede ayudar. Consideren eliminar las palabras comunes en inglés a través de las stop words. Todo este proceso se conoce como un count vectorizer, que significa transformar texto a vectores.

```{r}
dtm_long <- tokens %>%
  count(ID, word, name = "freq")

dtm_wide <- dtm_long %>%
  pivot_wider(
    names_from = word,
    values_from = freq,
    values_fill = 0
  ) %>%
  left_join(data %>% select(ID, Category) %>% distinct(), by = "ID")

```

```{r}
# Matriz dispersa y vector de etiquetas
X_sparse <- dtm_long %>%
  cast_sparse(row = ID, column = word, value = freq)

id_order <- as.integer(rownames(X_sparse))
y <- data %>%
  distinct(ID, Category) %>%
  arrange(match(ID, id_order)) %>%
  pull(Category)
```

```{r}
dim(dtm_wide)       # filas = relatos, columnas = palabras + Category
head(dtm_wide[, 1:10])  # primeras columnas para ver conteos

X_sparse            # resumen de la matriz dispersa
length(y)           # debe coincidir con nrow(X_sparse)
head(y)             # primeras etiquetas
```

✏️ Dividan la matriz de datos en training y test set. Y utilicen el training set para entrenar un clasificador naïve Bayes para predecir el tipo de evento paranormal.

✏️ Reporten la accuracy, precision, recall, F1-score y matriz de confusión con sus respectivas interpretaciones.

✏️ Intenten utilizar la función naive_bayes() del paquete naivebayes para volver a ajustar su clasificador, ¿existe alguna diferencia?

✏️ La clasificación multiclase impone un mayor desafío, para facilitar las cosas dicotomizaremos el tipo de evento paranormal. Con ayuda de la función case_when() pueden recodificar el tipo de evento paranormal, por ejemplo, si es Haunted Places u Other.

✏️ Vuelvan a entrenar un clasificador naïve Bayes con ambas funciones, ¿hubo alguna mejora?

✏️ Un problema que enfrentan estas primeras implementaciones del clasificador naïve Bayes es el supuesto de la distribución normal para los nodos hijo. Este supuesto parece poco pertinente en este caso debido a que lo que contiene nuestra matriz de datos son en realidad conteos. La distribución Poisson resulta una mejor alternativa para este tipo de datos. Pueden utilizar esta distribución en la función naive_bayes() del paquete naivebayes mediante el argumento usepoisson = TRUE.

✏️ Otro problema que enfrentamos en este tipo de datos es que existen muchos ceros en la matriz de datos. Una posible solución es utilizar una técnica de suavizamiento conocida como Laplace smoothing. Investiguen sobre esta técnica e impleméntenla en su clasificador.

✏️ Como se habrán dado cuenta, puede ser arbitraria la selección de este parámetro, intenten realizar cross-validation para seleccionar el valor óptimo.
